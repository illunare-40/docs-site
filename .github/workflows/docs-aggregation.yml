name: üìö Docs Aggregation & Deploy

on:
  schedule:
    # Executa a cada 30 minutos para manter docs sempre atualizados
    - cron: '*/30 * * * *'
  push:
    branches: [main]
    paths:
      - 'docs/**'
      - 'mkdocs.yml'
      - '.github/workflows/docs-aggregation.yml'
  pull_request:
    branches: [main]
    paths:
      - 'docs/**'
      - 'mkdocs.yml'
  workflow_dispatch:
    inputs:
      force_rebuild:
        description: 'For√ßar rebuild completo'
        required: false
        default: false
        type: boolean
      target_environment:
        description: 'Ambiente de deploy'
        required: false
        default: 'production'
        type: choice
        options:
          - production
          - staging
          - development

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'
  DOCS_SITE_URL: 'https://docs.illunare.com'
  GCP_PROJECT_ID: 'illunare-enterprise'
  GCP_REGION: 'us-central1'

jobs:
  # ==========================================
  # 1. DESCOBERTA DE REPOSIT√ìRIOS
  # ==========================================
  repository-discovery:
    name: üîç Repository Discovery
    runs-on: ubuntu-latest
    outputs:
      repositories: ${{ steps.discovery.outputs.repositories }}
      total_repos: ${{ steps.discovery.outputs.total_repos }}
    steps:
      - name: üîç Discover illunare-40 repositories
        id: discovery
        uses: actions/github-script@v7
        with:
          github-token: ${{ secrets.ORG_ACCESS_TOKEN }}
          script: |
            const repos = [];
            let page = 1;
            const perPage = 100;
            
            while (true) {
              const response = await github.rest.repos.listForOrg({
                org: 'illunare-40',
                type: 'all',
                sort: 'updated',
                per_page: perPage,
                page: page
              });
              
              if (response.data.length === 0) break;
              
              for (const repo of response.data) {
                // Incluir apenas reposit√≥rios com docs/ ou README.md atualizado recentemente
                if (repo.name.includes('-service') || 
                    repo.name.includes('-portal') || 
                    repo.name.includes('-app') ||
                    repo.name.includes('admin-') ||
                    repo.name.includes('mobile-') ||
                    repo.name.includes('landingpage-') ||
                    repo.name === 'docs-site') {
                  repos.push({
                    name: repo.name,
                    full_name: repo.full_name,
                    clone_url: repo.clone_url,
                    ssh_url: repo.ssh_url,
                    updated_at: repo.updated_at,
                    default_branch: repo.default_branch,
                    language: repo.language,
                    topics: repo.topics || []
                  });
                }
              }
              
              page++;
              if (response.data.length < perPage) break;
            }
            
            console.log(`Descobertos ${repos.length} reposit√≥rios`);
            core.setOutput('repositories', JSON.stringify(repos));
            core.setOutput('total_repos', repos.length);

  # ==========================================
  # 2. COLETA DE DOCUMENTA√á√ÉO
  # ==========================================
  docs-collection:
    name: üì• Docs Collection
    runs-on: ubuntu-latest
    needs: repository-discovery
    strategy:
      matrix:
        repo: ${{ fromJson(needs.repository-discovery.outputs.repositories) }}
      max-parallel: 10
      fail-fast: false
    steps:
      - name: üì• Clone repository ${{ matrix.repo.name }}
        uses: actions/checkout@v4
        with:
          repository: ${{ matrix.repo.full_name }}
          token: ${{ secrets.ORG_ACCESS_TOKEN }}
          path: ./repos/${{ matrix.repo.name }}
          fetch-depth: 1

      - name: üìã Extract documentation
        id: extract
        run: |
          REPO_PATH="./repos/${{ matrix.repo.name }}"
          OUTPUT_PATH="./extracted-docs/${{ matrix.repo.name }}"
          
          mkdir -p "$OUTPUT_PATH"
          
          # README principal
          if [[ -f "$REPO_PATH/README.md" ]]; then
            cp "$REPO_PATH/README.md" "$OUTPUT_PATH/index.md"
            echo "‚úÖ Extracted README.md"
          fi
          
          # Documenta√ß√£o em docs/
          if [[ -d "$REPO_PATH/docs" ]]; then
            cp -r "$REPO_PATH/docs/"* "$OUTPUT_PATH/" 2>/dev/null || true
            echo "‚úÖ Extracted docs/ directory"
          fi
          
          # API documentation
          if [[ -d "$REPO_PATH/api-docs" ]]; then
            mkdir -p "$OUTPUT_PATH/api"
            cp -r "$REPO_PATH/api-docs/"* "$OUTPUT_PATH/api/" 2>/dev/null || true
            echo "‚úÖ Extracted API docs"
          fi
          
          # OpenAPI specs
          find "$REPO_PATH" -name "*.yaml" -o -name "*.yml" -o -name "*.json" | \
            grep -E "(openapi|swagger|api-spec)" | while read file; do
            if [[ -f "$file" ]]; then
              mkdir -p "$OUTPUT_PATH/specs"
              cp "$file" "$OUTPUT_PATH/specs/"
              echo "‚úÖ Extracted spec: $(basename $file)"
            fi
          done
          
          # Dockerfile para entender stack
          if [[ -f "$REPO_PATH/Dockerfile" ]]; then
            mkdir -p "$OUTPUT_PATH/deployment"
            cp "$REPO_PATH/Dockerfile" "$OUTPUT_PATH/deployment/"
            echo "‚úÖ Extracted Dockerfile"
          fi
          
          # Package files para depend√™ncias
          for pkg_file in package.json go.mod Cargo.toml requirements.txt; do
            if [[ -f "$REPO_PATH/$pkg_file" ]]; then
              mkdir -p "$OUTPUT_PATH/dependencies"
              cp "$REPO_PATH/$pkg_file" "$OUTPUT_PATH/dependencies/"
              echo "‚úÖ Extracted $pkg_file"
            fi
          done
          
          # Metadata do reposit√≥rio
          cat > "$OUTPUT_PATH/.repo-metadata.json" << EOF
          {
            "name": "${{ matrix.repo.name }}",
            "full_name": "${{ matrix.repo.full_name }}",
            "language": "${{ matrix.repo.language }}",
            "topics": ${{ toJson(matrix.repo.topics) }},
            "updated_at": "${{ matrix.repo.updated_at }}",
            "extracted_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "default_branch": "${{ matrix.repo.default_branch }}"
          }
          EOF
          
          # Verificar se tem conte√∫do
          if [[ -n "$(find "$OUTPUT_PATH" -name "*.md" -type f)" ]]; then
            echo "has_docs=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Documenta√ß√£o extra√≠da para ${{ matrix.repo.name }}"
          else
            echo "has_docs=false" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è Nenhuma documenta√ß√£o encontrada para ${{ matrix.repo.name }}"
          fi

      - name: üì§ Upload extracted docs
        if: steps.extract.outputs.has_docs == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: docs-${{ matrix.repo.name }}
          path: ./extracted-docs/${{ matrix.repo.name }}
          retention-days: 1

  # ==========================================
  # 3. AGREGA√á√ÉO E BUILD
  # ==========================================
  docs-aggregation:
    name: üîß Docs Aggregation
    runs-on: ubuntu-latest
    needs: [repository-discovery, docs-collection]
    steps:
      - name: üì• Checkout docs-site
        uses: actions/checkout@v4
        with:
          path: ./docs-site

      - name: üêç Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: üì¶ Install dependencies
        working-directory: ./docs-site
        run: |
          pip install -r requirements.txt

      - name: üì• Download all extracted docs
        uses: actions/download-artifact@v4
        with:
          path: ./extracted-docs
          pattern: docs-*

      - name: üîß Generate navigation and aggregate docs
        id: aggregate
        run: |
          cd docs-site
          
          # Script Python para agrega√ß√£o
          cat > aggregate_docs.py << 'EOF'
          import os
          import json
          import yaml
          import shutil
          from pathlib import Path
          from datetime import datetime
          
          def load_repo_metadata(docs_path):
              metadata_file = docs_path / '.repo-metadata.json'
              if metadata_file.exists():
                  with open(metadata_file) as f:
                      return json.load(f)
              return {}
          
          def categorize_service(repo_name, metadata):
              """Categoriza servi√ßos baseado no nome e metadata"""
              name = repo_name.lower()
              language = metadata.get('language', '').lower()
              topics = metadata.get('topics', [])
              
              if 'admin-portal' in name or 'mobile-app' in name or 'landingpage' in name:
                  return 'frontend'
              elif any(x in name for x in ['ai-', 'ml-', 'deepseek', 'ollama', 'cortex']):
                  return 'ai'
              elif any(x in name for x in ['security', 'auth', 'biometric', 'fraud']):
                  return 'security'
              elif any(x in name for x in ['db-', 'storage', 'data-', 'datomic']):
                  return 'data'
              elif any(x in name for x in ['gateway', 'integration', 'connector']):
                  return 'integration'
              else:
                  return 'core'
          
          def generate_service_index(services_by_category):
              """Gera index dos servi√ßos por categoria"""
              content = """# üîß Servi√ßos da Plataforma illunare 4.0
          
          Este √© o √≠ndice completo de todos os microservi√ßos e componentes da plataforma illunare 4.0 Enterprise.
          
          """
              
              # Estat√≠sticas
              total_services = sum(len(services) for services in services_by_category.values())
              content += f"üìä **Total de Servi√ßos**: {total_services}\n\n"
              
              categories = {
                  'frontend': 'üé® Frontend Applications',
                  'ai': 'ü§ñ AI & Machine Learning',
                  'security': 'üîê Security Services', 
                  'data': 'üìä Data Services',
                  'integration': 'üîó Integration Services',
                  'core': '‚öôÔ∏è Core Services'
              }
              
              for category, title in categories.items():
                  if category in services_by_category and services_by_category[category]:
                      content += f"## {title}\n\n"
                      
                      for service in sorted(services_by_category[category], key=lambda x: x['name']):
                          metadata = service['metadata']
                          name = service['name']
                          language = metadata.get('language', 'Unknown')
                          updated = metadata.get('updated_at', '')
                          if updated:
                              updated = datetime.fromisoformat(updated.replace('Z', '+00:00')).strftime('%d/%m/%Y')
                          
                          # Status badge baseado na linguagem
                          lang_badge = {
                              'typescript': '![TypeScript](https://img.shields.io/badge/TypeScript-007ACC?style=flat&logo=typescript&logoColor=white)',
                              'javascript': '![JavaScript](https://img.shields.io/badge/JavaScript-F7DF1E?style=flat&logo=javascript&logoColor=black)',
                              'go': '![Go](https://img.shields.io/badge/Go-00ADD8?style=flat&logo=go&logoColor=white)',
                              'rust': '![Rust](https://img.shields.io/badge/Rust-000000?style=flat&logo=rust&logoColor=white)',
                              'python': '![Python](https://img.shields.io/badge/Python-3776AB?style=flat&logo=python&logoColor=white)',
                              'elixir': '![Elixir](https://img.shields.io/badge/Elixir-4B275F?style=flat&logo=elixir&logoColor=white)',
                              'dart': '![Dart](https://img.shields.io/badge/Dart-0175C2?style=flat&logo=dart&logoColor=white)'
                          }.get(language.lower(), f'![{language}](https://img.shields.io/badge/{language}-gray?style=flat)')
                          
                          content += f"### [{name}]({category}/{name}/index.md)\n\n"
                          content += f"{lang_badge} "
                          content += f"üìÖ {updated}\n\n"
                          
                          # Descri√ß√£o se dispon√≠vel
                          if 'description' in metadata:
                              content += f"{metadata['description']}\n\n"
                          
                          content += "---\n\n"
              
              return content
          
          # Diret√≥rio principal de docs
          docs_dir = Path('docs')
          services_dir = docs_dir / 'services'
          
          # Criar estrutura de diret√≥rios
          for category in ['core', 'ai', 'data', 'security', 'integration', 'frontend']:
              (services_dir / category).mkdir(parents=True, exist_ok=True)
          
          # Processar documenta√ß√µes extra√≠das
          extracted_base = Path('../extracted-docs')
          services_by_category = {}
          
          for docs_artifact in extracted_base.iterdir():
              if docs_artifact.is_dir() and docs_artifact.name.startswith('docs-'):
                  repo_name = docs_artifact.name[5:]  # Remove 'docs-' prefix
                  
                  # Carregar metadata
                  metadata = load_repo_metadata(docs_artifact)
                  category = categorize_service(repo_name, metadata)
                  
                  if category not in services_by_category:
                      services_by_category[category] = []
                  
                  services_by_category[category].append({
                      'name': repo_name,
                      'metadata': metadata,
                      'path': docs_artifact
                  })
                  
                  # Copiar documenta√ß√£o para categoria apropriada
                  target_dir = services_dir / category / repo_name
                  if target_dir.exists():
                      shutil.rmtree(target_dir)
                  
                  shutil.copytree(docs_artifact, target_dir)
                  print(f"‚úÖ Copiado {repo_name} para {category}/")
          
          # Gerar √≠ndice de servi√ßos
          services_index = generate_service_index(services_by_category)
          with open(services_dir / 'index.md', 'w') as f:
              f.write(services_index)
          
          # Atualizar mkdocs.yml com navega√ß√£o din√¢mica
          with open('mkdocs.yml', 'r') as f:
              mkdocs_config = yaml.safe_load(f)
          
          # Atualizar se√ß√£o de servi√ßos na navega√ß√£o
          for item in mkdocs_config['nav']:
              if isinstance(item, dict) and 'üîß Servi√ßos' in item:
                  services_nav = [
                      'services/index.md'
                  ]
                  
                  # Adicionar categorias
                  category_titles = {
                      'frontend': 'üé® Frontend Applications',
                      'ai': 'ü§ñ AI Services',
                      'security': 'üîê Security Services',
                      'data': 'üìä Data Services',
                      'integration': 'üîó Integration Services',
                      'core': '‚öôÔ∏è Core Services'
                  }
                  
                  for category, title in category_titles.items():
                      if category in services_by_category:
                          services_nav.append(f'services/{category}/')
                  
                  item['üîß Servi√ßos'] = services_nav
                  break
          
          # Salvar mkdocs.yml atualizado
          with open('mkdocs.yml', 'w') as f:
              yaml.dump(mkdocs_config, f, default_flow_style=False, allow_unicode=True)
          
          print(f"‚úÖ Agrega√ß√£o conclu√≠da: {sum(len(services) for services in services_by_category.values())} servi√ßos processados")
          EOF
          
          python aggregate_docs.py

      - name: üèóÔ∏è Build MkDocs site
        working-directory: ./docs-site
        run: |
          mkdocs build --strict --verbose

      - name: üìä Generate build report
        working-directory: ./docs-site
        run: |
          echo "# üìä Build Report - $(date)" > build-report.md
          echo "" >> build-report.md
          echo "## üìà Estat√≠sticas" >> build-report.md
          echo "- **Total de p√°ginas**: $(find site -name "*.html" | wc -l)" >> build-report.md
          echo "- **Tamanho do site**: $(du -sh site | cut -f1)" >> build-report.md
          echo "- **Reposit√≥rios processados**: ${{ needs.repository-discovery.outputs.total_repos }}" >> build-report.md
          echo "- **Tempo de build**: $(date)" >> build-report.md
          echo "" >> build-report.md
          echo "## üîó Links" >> build-report.md
          echo "- **Site**: ${{ env.DOCS_SITE_URL }}" >> build-report.md
          echo "- **Reposit√≥rio**: https://github.com/illunare-40/docs-site" >> build-report.md

      - name: üì§ Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: docs-site-build
          path: ./docs-site/site/
          retention-days: 7

      - name: üì§ Upload build report
        uses: actions/upload-artifact@v4
        with:
          name: build-report
          path: ./docs-site/build-report.md
          retention-days: 7

  # ==========================================
  # 4. DEPLOY
  # ==========================================
  deploy:
    name: üöÄ Deploy to GitHub Pages
    runs-on: ubuntu-latest
    needs: docs-aggregation
    if: github.ref == 'refs/heads/main'
    environment:
      name: production
      url: ${{ env.DOCS_SITE_URL }}
    permissions:
      contents: read
      pages: write
      id-token: write
    steps:
      - name: üì• Download build artifacts
        uses: actions/download-artifact@v4
        with:
          name: docs-site-build
          path: ./site

      - name: üìÑ Setup Pages
        uses: actions/configure-pages@v4

      - name: üì§ Upload to Pages
        uses: actions/upload-pages-artifact@v3
        with:
          path: ./site

      - name: üöÄ Deploy to Pages
        id: deployment
        uses: actions/deploy-pages@v4

  # ==========================================
  # 5. NOTIFICA√á√ÉO
  # ==========================================
  notification:
    name: üì¢ Notification
    runs-on: ubuntu-latest
    needs: [repository-discovery, docs-collection, docs-aggregation, deploy]
    if: always()
    steps:
      - name: üìä Calculate success rate
        id: stats
        run: |
          TOTAL_REPOS=${{ needs.repository-discovery.outputs.total_repos }}
          SUCCESSFUL_JOBS=$(echo '${{ toJson(needs.docs-collection.result) }}' | grep -o 'success' | wc -l || echo "0")
          SUCCESS_RATE=$((SUCCESSFUL_JOBS * 100 / TOTAL_REPOS))
          
          echo "total_repos=$TOTAL_REPOS" >> $GITHUB_OUTPUT
          echo "successful_jobs=$SUCCESSFUL_JOBS" >> $GITHUB_OUTPUT
          echo "success_rate=$SUCCESS_RATE" >> $GITHUB_OUTPUT

      - name: üì¢ Success notification
        if: needs.deploy.result == 'success'
        run: |
          echo "‚úÖ Documenta√ß√£o atualizada com sucesso!"
          echo "üìä Reposit√≥rios processados: ${{ steps.stats.outputs.successful_jobs }}/${{ steps.stats.outputs.total_repos }}"
          echo "üìà Taxa de sucesso: ${{ steps.stats.outputs.success_rate }}%"
          echo "üîó Site: ${{ env.DOCS_SITE_URL }}"

      - name: ‚ö†Ô∏è Failure notification
        if: failure()
        run: |
          echo "‚ùå Falha na atualiza√ß√£o da documenta√ß√£o"
          echo "üìä Status dos jobs:"
          echo "  - Repository Discovery: ${{ needs.repository-discovery.result }}"
          echo "  - Docs Collection: ${{ needs.docs-collection.result }}"
          echo "  - Docs Aggregation: ${{ needs.docs-aggregation.result }}"
          echo "  - Deploy: ${{ needs.deploy.result }}" 